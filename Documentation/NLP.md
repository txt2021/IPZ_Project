### Що таке NLP?

Подальшу інформацію було взято з <https://habr.com/ru/company/abbyy/blog/437008/>

Natural Language Processing, як правило, скорочена як NLP - це галузь штучного інтелекту, яка займається взаємодією між комп'ютерами та людьми за допомогою природної мови.
Кінцева мета NLP - читати, розшифровувати, розуміти та осмислювати людські мови.
Більшість методик NLP покладаються на машинне навчання, щоб отримувати значення з людських мов.

### Для чого використовується NLP?

NLP застосовується у:

  * програмах перекладу мов, таких як Google Translate;
  * пекстових процесорах, які використовують NLP для перевірки граматичної точності текстів. Наприклад Microsoft Word та Grammarly;
  * програмах інтерактивної голосової відповіді (IVR), що використовуються в телефонних центрах для відповіді на запити певних користувачів.
  * персональних програмах-помічниках, таких як OK Google, Siri, Cortana та Alexa.

### Чому вирішувати задачі NLP складно?

Формулювання завдань не дуже складні, проте самі завдання зовсім не є простими, тому що ми працюємо з природною мовою. Явища полісемії (багатозначні слова мають загальний вихідний сенс) і омонімії (різні за змістом слова вимовляються і пишуться однаково) характерні для будь-якого природної мови.

Полісемія: зупинка (процес або будівля), стіл (організація або об'єкт), дятел (птах або людина).
Омонімія: ключ, цибуля, замок, піч.

Іншим класичним прикладом складності мови є займенникова анафора. Наприклад, нехай нам дано текст «Двірник дві години прибирав сніг, він був незадоволений». Займенник «він» може ставитися як до двірника, так і до снігу. По контексту ми легко зрозуміємо, що він - це двірник, а не сніг. Але домогтися, щоб комп'ютер це теж легко розумів, непросто. Завдання займенникової анафори і зараз вирішено не дуже добре, тривають активні спроби поліпшити якість рішень.
Ще одна додаткова складність - це еліпсис. Наприклад, «Петрик з'їв зелене яблуко, а Маша - червоне». Ми розуміємо, що Маша з'їла червоне яблуко. Проте, домогтися, щоб машина теж це зрозуміла, непросто. Зараз завдання відновлення вирішується на крихітних корпусах (кілька сотень пропозицій), і на них якість повного відновлення відверто слабка (близько 0.5). Зрозуміло, що для практичних застосувань така якість нікуди не годиться. 

### Як працює обробка природних мов?

Подальшу інформацію було взято з <https://habr.com/ru/company/Voximplant/blog/446738/>

NLP тягне за собою застосування алгоритмів для ідентифікації та вилучення правил природної мови таким чином, що неструктуровані мовні дані перетворювались у форму, зрозумілу комп'ютерам.
Сирий текст не підходить для навчання машин. Потрібно конвертувати текст у вектори – набори цифр. Цей процес називають виділенням ознак.

Мішок слів – техніка виділення ознак, яка описує входження слів у текст. 

Для використання моделі необхідно визначити словник відомих слів, які називають токенами, а також обрати ступінь їхньої присутності. 

#### Приклад

Розглянемо створення моделі, використовуючи приклад для наочності.

1.	Загрузка даних
   
      У нас є дані:
      
      I like this movie, it's funny.  
      I hate this movie.  
      This was awesome! I like it.  
      Nice one. I love it.  
      Загружаємо їх у вигляді масиву: ["I like this movie, it's funny.", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']  

2.	Визначаємо словник

Збираємо усі унікальні слова, ігноруючи реєстр, пунктуацію, токени з одного символу.

3.	Створюємо вектори документа

Перетворюємо сирий текст у набір цифр, щоб використовувати їх як вхідні дані для моделі машинного навчання.
Відмічаємо наявність слів (1 – є, 0 – немає)
Результат:

|      | awesome | funny | hate | it   | like | love | movie | nice | one  | this | was  |
| ---: |--------:| -----:|-----:| ---: | ---: | ---: | ----: | ---: | ---: | ---: | ---: |
| 0    | 0       | 1     | 0    | 1    | 1    | 0    | 1     | 0    | 0    | 1    | 0    |
| 1    | 0       | 0     | 1    | 0    | 0    | 0    | 1     | 0    | 0    | 1    | 0    |
| 2    | 1       | 0     | 0    | 1    | 1    | 0    | 0     | 0    | 0    | 1    | 1    |
| 3    | 0       | 0     | 0    | 1    | 0    | 1    | 0     | 1    | 1    | 0    | 0    |

Із словником росте вектор документу. У прикладі довжина вектора дорівнює кількості відомих слів. Якщо вектор складається з тисяч або мільйонів елементів, кожен документ може містити лише меншу частину слів їх словника. Виходить вектор з багатьма нулями, який вимагає багато пам’яті та ресурсів.

Можна зменшити кількість слів, щоб зменшити вимоги до обчислювальних ресурсів.
Наприклад, викинути стоп-слова, привести слова до базових форм і виправити неправильно написані слова.
Інший спосіб – використання згрупованих слів, інакше кажучи, N-грам (N – кількість згрупованих слів). У модель потрапляють лише ті, що фігурують у корпусі.

#### Приклад:

Речення: The office building is open today

Його біграми (сполучення двох слів):
  * the office
  * office building
  * building is
  * is open
  * open today

Ми оцінили наявність слів, але також можна рахувати їх кількість у документі та частоту відносно загальної кількості.
Коли текст буде надано, комп'ютер буде використовувати алгоритми, щоб витягнути значення, пов'язане з кожним реченням, та зібрати з них суттєві дані.
Іноді комп'ютер може не зрозуміти значення речення, що призводить до незрозумілих результатів.

### Які методи використовуються в НЛП?

Синтаксичний аналіз та семантичний аналіз - основні прийоми, що використовуються для виконання завдань з обробки природних мов.

1. Синтаксис

      Синтаксис позначає розташування слів у реченні таким чином, що вони мають граматичний зміст.  
      У NLP синтаксичний аналіз використовується для оцінки того, як природна мова узгоджується з граматичними правилами.  
      Комп'ютерні алгоритми використовуються для застосування граматичних правил до групи слів та отримання значень від них.

      Ось кілька синтаксичних прийомів:
     * Лематизація: це тягне за собою зведення різних складних форм слова в єдину форму для легкого аналізу.
     * Морфологічна сегментація: передбачає поділ слів на окремі одиниці, що називаються морфемами.
     * Сегментація слова: передбачає поділ великого фрагмента безперервного тексту на окремі одиниці.
     * Позначення частин мови: передбачає визначення частини мови для кожного слова.
     * Розбір: Він передбачає проведення граматичного аналізу поданого речення.
     * Позначення кінця речення: передбачає розміщення меж речення на великому фрагменті тексту.
     * Стовбування: Це передбачає вирізання схильних слів до їх кореневої форми.

2. Семантика

      Семантика позначає значення, яке передається текстом. Семантичний аналіз - один із складних аспектів обробки природних мов, який ще не був повністю вирішений.
      Він передбачає застосування комп’ютерних алгоритмів для розуміння значення та інтерпретації слів та структури речень.

     Ось деякі методи семантичного аналізу:
     + Розпізнавання іменованої сутності (NER): включає визначення частин тексту, які можна ідентифікувати та класифікувати за попередньо заданими групами. Приклади таких груп включають імена людей та назви місць.
     - Розбіжність сенсу у слові: передбачає надання значення слову на основі контексту.
     - Генерація природних мов: передбачає використання баз даних для отримання смислових намірів та перетворення їх на людську мову.

### NER

Подальшу інформацію було взято з <https://habr.com/ru/company/abbyy/blog/449514/>

Витяг іменованих сутностей (Named-entity recognition, NER) - одна з найпопулярніших завдань NLP.
Завдання NER - визначити частини тексту, які можна ідентифікувати та класифікувати за попередньо заданими групами. Приклади таких груп включають імена людей та назви місць.

Припустимо, є новинний текст, і ми хочемо виділити в ньому сутності (деякий заздалегідь зафіксований набір - наприклад, персони, локації, організації, дати і так далі). Завдання NER - зрозуміти, що фрагмент тексту «1 січня 1997 року" є датою, "Кофі Аннан" - персоною, а "ООН" — організацією.

#### Зведення задачі NER до задачі класифікації

Незважаючи на те що сутності часто бувають багатослівними, зазвичай завдання NER зводиться до задачі класифікації на рівні токенов, т. Е. Кожен токен відноситься до одного з декількох можливих класів. Є кілька стандартних способів зробити це, але самий загальний з них називається BIOES-схемою. Схема полягає в тому, щоб до мітки суті (наприклад, PER для персон або ORG для організацій) додати деякий префікс, який позначає позицію токена в спання суті. Більш детально:

   B - від слова beginning - перший токен в спання суті, який складається з більше ніж 1 слова.  
   I - від словами inside - це те, що знаходиться в середині.  
   E - від слова ending, це останній токен суті, яка складається більше ніж з 1 елемента.  
   S - single. Ми додаємо цей префікс, якщо сутність складається з одного слова.
   
Таким чином, до кожного типу сутності додаємо один з 4 можливих префіксів. Якщо токен не відноситься ні до якої сутності, він позначається спеціальною міткою, зазвичай має позначення OUT або O.

Зрозуміло, що за такої розмітки ми однозначно можемо встановити межі всіх анотацій сутностей. Дійсно, про кожен токен ми знаємо, чи вірно, що сутність починається з цього токена або закінчується на ньому, а значить, закінчити чи анотацію суті на даному токені, або розширювати її на наступні маркери.

Переважна більшість дослідників використовує цей спосіб (або його варіації з меншою кількістю міток - BIOE або BIO), але у нього є кілька суттєвих недоліків. Головний з них полягає в тому, що схема не дозволяє працювати з вкладеними або пересічними сутностями. Наприклад, сутність "МГУ імені М.В. Ломоносова "- це одна організація. Але Ломоносов сам по собі - це персона, і це теж було б непогано поставити в розмітці. За допомогою описаного вище способу розмітки ми ніколи не зможемо передати обидва ці факти одночасно (бо у одного токена можемо зробити тільки одну позначку). Відповідно, токен "Ломоносова" може бути або частиною анотації організації, або частиною анотації персони, але ніколи не тим і іншим одночасно.

Інший приклад вкладених сутностей: "Кафедра математичної логіки і теорії алгоритмів механіко-математичного факультету МДУ". Тут в ідеалі хотілося б виділяти 3 вкладених організації, але наведений вище спосіб розмітки дозволяє виділити або 3 непересічні суті, або одну сутність, що має анотацією весь наведений фрагмент.

Крім стандартного способу звести задачу до класифікації на рівні токенов, є і стандартний формат даних, в якому зручно зберігати розмітку для завдання NER (а також для багатьох інших завдань NLP). Цей формат називається CoNLL-U.

### Як вирішити задачу NER за допомогою нейромереж?

Автори вводять два різновиди архітектури, що відповідають двом різним способам врахувати контекст токена:
  * або використовувати «вікно» заданої ширини (window based approach),
  * або вважати контекстом все пропозицію (sentence based approach).
  
Ми отримали на вхід список слів нашої пропозиції: наприклад, "The cat sat on the mat".
Нехай все мається K різних ознак для одного токена (наприклад, такими ознаками можуть виступати словоформа, частина мови, капіталізація, чи є наш токен першим або останнім в реченні і т. П.). Всі ці ознаки ми можемо вважати категоріальним (наприклад, словоформи відповідає логічний вектор довжини розмірності словника, де 1 стоїть тільки на координаті відповідної індексу слова в словнику). Нехай image - логічний вектор, відповідний значенням i-ї ознаки j-го токена в реченні.

Важливо відзначити, що в sentence based approach крім категоріальних ознак, що визначаються за словами, використовується ознака - зрушення щодо токена, мітку якого ми намагаємося визначити. Значення цієї ознаки для токена номер i буде i-core, де core - номер токена, мітку якого ми намагаємося визначити в даний момент (ця ознака теж вважається категоріальним, і вектора для нього обчислюються точно так же, як і для інших).

Наступний етап знаходження ознак токена - множення кожного image на матрицю image, яка називається Lookup Table (таким чином булеві вектора "перетворюються" в безперервні). Нагадаємо, що кожен з image - логічний вектор, в якому на одному місці стоїть 1, а на інших місцях - 0. Таким чином при множенні image на image, відбувається вибір однієї з рядків в нашій матриці. Цей рядок і є ембеддінгом відповідної ознаки токена. Матриці image (де i може набувати значень від 1 до K) - це параметри нашої мережі, які ми навчаємо разом з іншими верствами нейромережі.

Відмінність описаного  способу роботи з категоріальним ознаками від появи пізніше word2vec в тому, що тут матриці ініціалізуються випадковим чином, а в word2vec матриці перевчатються на великому корпусі на задачі визначення слова по контексту (або контексту по слову).

Таким чином, для кожного токена отриманий безперервний вектор ознак, що є конкатенацией результатів перемноження всіляких image на image.

Тепер розберемося з тим, як ці ознаки використовуються в sentence based approach (window based ідейно простіше). Важливо, що ми будемо запускати нашу архітектуру окремо для кожного токена (т. Е. Для пропозиції "The cat sat on the mat" ми запустимо нашу мережу 6 разів). Ознаки в кожному запуску збираються однакові, за винятком ознаки, що відповідає за позицію токена, мітку якого ми намагаємося визначити - токена core.

Беремо отримані безперервні вектора кожного токена і пропускаємо їх через одновимірну згортку з фільтрами не дуже великий розмірності: 3-5. Розмірність фільтра відповідає розміру контексту, який мережу одночасно враховує, а кількість каналів відповідає розмірності вихідних безперервних векторів (сумі розмірностей ембеддінгов всіх ознак). Після застосування згортки одержуємо матрицю розмірності m на f, де m - кількість способів, якими фільтр можна прикласти до нашими даними (т. Е. Довжина пропозиції мінус довжина фільтра плюс один), а f - кількість використовуваних фільтрів.

Як і в більшості випадків при роботі з пакунками, після згортки ми використовуємо пулінг - в даному випадку max pooling (т. Е. Для кожного фільтра беремо максимум його значення на всьому реченні), після чого отримуємо вектор розмірності f. Таким чином, вся інформація, що міститься в реченні, яка може нам знадобитися при визначенні мітки токена core, стискається в один вектор (max pooling був обраний тому, що нам важлива не інформація в середньому за пропозицією, а значення ознак на його найважливіших ділянках) . Такий "сплюснутий контекст" дозволяє нам збирати ознаки нашого токена по всьому пропозицією і використовувати цю інформацію, щоб визначити, яку мітку повинен отримати токен core.

Далі пропускаємо вектор через багатошаровий персептрон з якимись функціями активації (в статті - HardTanh), а в якості останнього шару використовуємо повнозв'язну з softmax розмірності d, де d - кількість можливих міток токена.
Таким чином сверточних шар дозволяє нам зібрати інформацію, що міститься у вікні розмірності фільтра, пулінг - виділити саму характерну інформацію в реченні (стиснувши її в один вектор), а шар з softmax - дозволяє визначити, яку ж мітку має токен номер core.
 
 ### Сервіси, що реалізують NLP
 
 #### Textrazor
 
   В мережі існують певні сервіси, наприклад [textrazor.com](https://www.textrazor.com), який надає можливість безкоштовно протестувати можливості NLP (demo), та має кілька функціональних переваг. Подальшу інформацію було взято з <https://www.textrazor.com/technology>

* TextRazor використовує сучасні методи обробки природних мов та штучного інтелекту для аналізу, аналізу та вилучення семантичних метаданих із заданого вмісту.

* API TextRazor можна легко інтегрувати з будь-якою мовою, яка може надіслати HTTP-запит і проаналізувати відповідь JSON, завдяки чому можлива потужна аналітика тексту лише за допомогою декількох рядків коду. TextRazor дозволяє витягти будь-яку та всю необхідну інформацію в одному запиті, пов'язуючи витягнуті семантичні метадані, щоб спростити ідентифікацію складних шаблонів.

* Великі дані корисні лише в тому випадку, якщо користувацьке програмне забезпечення може йти в ногу з цим. TextRazor був розроблений з нуля для продуктивності. Написаний на сильно оптимізованому C ++, TextRazor здатний обробляти тисячі слів в секунду. Сервіс створений для автоматичного масштабування до мільярдів запитів у хмарі. 

* Стійка інфраструктура TextRazor побудована на хмарі Amazon Web Services і фізичному обладнанні. TextRazor розроблений для забезпечення високої доступності та послідовності продуктивності для аналізу тисяч, мільйонів або мільярдів щоденних документів.

* TextRazor дозволяє додавати назви продуктів, людей, компаній, спеціальні правила класифікації та вдосконалені мовні зразки. Інтегрований двигун Prolog дозволяє швидко поєднувати результати TextRazor з надійною логікою, призначеною для користувача. 

#### Наташа

Подальшу інформацію було взято з <https://habr.com/ru/post/349864/>

Для витягування слів з російського тексту рішень небагато. Є рішення у Томіта-парсері, але там інтеграція з Python незручна. Також з iPavlov є рішення, але імена не зводяться до нормальної форми. З витягуванням адрес або посилань на нормативні акти ще складніше.

Наташа – аналог Томіта-парсера для Python і набір готових правил для витягування імен, адрес, дат, сум грошей та інших сутностей. Можна додавати свої правила за допомогою Yargy-парсера.

Зараз є правила для витягування імен, адрес, дат і сум грошей.  Правила для назв організацій і географічних об’єктів нижчої якості. 

У 2016 році проводилося змагання factRuEval-2016 з витягування іменних сутностей. Найкраща F1-міра була більше 0.9. У Наташі результат 0.78 через проблеми з іноземними іменами та складними прізвищами. Для текстів з російськими іменами результат ~0.95.

Проблемою бібліотеки Наташа є обмеженість готовими правилами. Наприклад, не вдасться визначити ««1» липня 2018» як дату, бо в правилах не враховані лапки. Також не вдасться розібрати адресу без назви вулиці.
У таких випадках доводиться доповнювати готові правила та писати свої за допомогою Yargy-парсеру, бібліотеці, яка є основою Наташі.

Yargy-парсер – аналог яндексового Томіта-парсера для Python. Правила для витягування сутностей описуються за допомогою контекстно-вільних граматик і словників.
Граматики в Yargy записуються на спеціальному DSL-і. Застосовуються вони для таких задач як витягування дат в ISO-форматі («2018-03-04», «2014-04-28»).

У парсер вбудовано багато готових предикантів і є можливість додати свої. Для визначення морфології слів використовується pymorphy2.

Парсер також виконує інтерпрепацію, і, наприклад, замість «16 серпня» повертає Date(month=8, day=16). Результат роботи парсера – дерево розбору.
 
Для інтерпретації користувач розвішує на вузли дерева помітки.
 
У прикладі потрібно звести назви років, місяців і днів до числам. Ця процедура називається нормалізацією.
Наприклад, січень – 1, лютий – 2, березень – 3 і так далі.

Також у Yargy є механізм узгодження, наприклад, імен і прізвищ за родом, числом та відмінком.

Рішення Наташі під ліцензією MIT, тобто, можна вільно використовувати, модифікувати і так далі.

Недоліками Наташі є потреба вручну складати правила та повільна швидкість (наприклад, витягування імен у 10 разів повільніше ніж у Томіта-парсера). Також наявні помилки у стандартних правилах.

Адреса проекту на Github — <https://github.com/natasha>.

### ВЕСУМ-великий електронний словник української мови

Подальшу інформацію було взято з <https://r2u.org.ua/articles/vesum>

Бере початок з проекту ispell-uk, що його в 90-х роках створила група ентузіастів для перевірки орфографії української мови у відкритій ОС Linux. Багато років цей словник мав єдину функцію — перевіряти орфографію текстів. Але декілька років тому в інший відкритий проект, програму перевірки граматики та стилю LanguageTool, було додано модуль української мови

До проекту долучилася команда створення відкритого корпусу української мови БрУК

Для створення ВЕСУМ-а ми використали два найголовніших джерела: “Граматичний словник української літературної мови. Словозміна” (опублікований 2011 року й удоступнений на Лінгвістичному порталі (<http://www.mova.info/grmasl.aspx>) та “Словники України” онлайн (<http://lcorp.ulif.org.ua/dictua/>).

#### Зусилля, вкладені в проект, на виході дали без перебільшення унікальний словник:

Основні застосування:

  * налічує понад 285 тис. лем і постійно поповнюється

  * містить інформацію про відмінювання слів

  * подає нерекомендовані слова (активні дієприкметники, невдалі кальки тощо) та заміну для них

  * охоплює абревіатури та скорочення

  * містить інформацію про деякі альтернативні правописні варіанти (дає змогу аналізувати тексти, написані не за чинним правописом, адже низка медій, авторів, видавництв свідомо дотримуються альтернативних правописних правил)

  * має велику базу власних імен (зокрема українських імен, по батькові та прізвищ, неукраїнських імен та прізвищ, українських та закордонних топонімів тощо)

  * синхронізований з КОАТУУ, зокрема містить назви, що з’явилися внаслідок декомунізації

  * має дуже компактну систему позначення відмінювання та тегів для слів, завдяки чому легко додавати нові слова, групувати наявні тощо

  * містить інформацію про деякі рідкісні та розмовні форми, наприклад, нестягнені форми прикметників (гарная), та розмовні інфінітиви (поїхать); щоправда, більшість таких форм вимкнено за уставою, оскільки вони мають обмежену форму застосування й часто створюють зайву омонімію (однак за потреби їх можна ввімкнути)

  * є відкритим проектом (розміщений на github), тож кожен може долучитися до вдосконалення та використовувати його у своїй роботі.

Інші застосування

  * укладання тлумачних, термінологічних, перекладних та інших типів словників (зокрема пошук прикладів вживання)

  * різноманітні мовознавчі дослідження

  * дослідження і розробки у галузі комп’ютерної лінгвістики (зокрема побудова моделей мови, отримання статистичної інформації)

  * довідкові функції та редагування

#### Структура словника

Словник містить три основні частини:
  1. правила зміни суфіксів у парадигмах
  2. слова з прапорцями парадигм та додаткових властивостей
  3. код генерування повних парадигм із сирцевої інформації (1 та 2)

Наприклад, запис тракторист /n20.a.p. – означає, що це відмінюване слово чоловічого роду другої відміни без чергування (і/о), істота, з закінченням -а в родовому відмінку.

### Де застосовується NLP?

Наведена нижче інформація була взята з цього ресурсу : https://evergreens.com.ua/ru/articles/natural-language-processing.html

Якщо вам здається, що ви ніколи не стикалися з NLP, то досить відкрити Google, біля пошукового рядка натиснути на значок мікрофона і сказати: «Окей, гугл ...». І ось пошукова система обробляє потрібний вам запит.

Але ця функція була б недоступна, якби не можливість пристрою зрозуміти природну мову, якою розмовляють люди. Здатність машини обробляти сказане, структурувати отриману інформацію, визначати необхідну відповідь дія і відповідати на мові, зрозумілій користувачеві, і є NLP або Natural Language Processing.

#### Чатбот

NLP стало основою для створення чатботу. Слід сказати, що за допомогою чатботу вирішується проблема завантаженості колл-центрів і приймальних відділень. Наприклад, після впровадження компанією "Київстар" чатботу Зоряна у 2016 році, завантаження операторів значно знизилася. Завдяки великій базі в 12 000 стандартизованих відповідей, бот допомагає з рішенням 70% входять питань. Це підтверджує ефективність використання чатботу для великих компаній.

Багато організацій потребують NLP як в помічника для структурування даних. Адже на сьогодні ще існує завдання оцифровки інформації. І тут ми знову звертаємося до обробника природної мови, який аналізує документацію і класифікує її.Використовують NLP і в медицині для поліпшення обслуговування пацієнтів, ведення медичних карт і пошуку ключових термінів у фаховій літературі. З його допомогою реалізовані лікарі-роботи, які зіставляють симптоми хворих з відповідними діагнозами і відстежують перебіг хвороби.Більш того, можливості обробки даних і прогнозування дозволяють використовувати NLP для запобігання злочинів. Застосовуючи її, поліція може аналізувати злочинну діяльність, обчислювати кодові слова злочинців в рекламі і швидше реагувати щоб уникнути насильства і торгівлі людьми. І це, напевно, найбільш вражаюче застосування NLP на сьогодні.

### Бібліотеки для NPL

Подальшу інформацію було взято з <http://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D1%8F%D0%B7%D1%8B%D0%BA%D0%B0>

#### NLTK (Natural Language ToolKit)

NLTK - це провідна платформа для побудови програм Python для роботи з даними людської мови. Вона надає прості у використанні інтерфейси для понад 50 корпоративних та лексичних ресурсів, таких як WordNet, а також набір бібліотек для обробки тексту для класифікації, токенізації, стримування, розмітки тегів, розбору та семантичних міркувань.

Плюси:
  * Найвідоміша і повна по функціоналу бібліотека для NLP;
  * Велика кількість сторонніх розширень;
  * Швидка токенізація пропозицій;
  * Підтримує безліч мов.

Мінуси:
  * повільна;
  * Складна в вивченні і використанні;
  * Працює з рядками;
  * Не використовує нейронні мережі;
  * Не має  вбудованих векторів слів.

#### spaCy 

Бібліотека, написана на мові Cypthon, позиціонується як найшвидша NLP бібліотека. Має безліч можливостей, в тому числі, розбір залежностей на основі міток, розпізнавання іменованих сутностей, позначення частин мови, вектори розстановки слів. Не підтримує російську мову.

Плюси:
  * Найшвидша бібліотека для NLP;
  * Проста у вивченні і використанні;
  * Працює з об'єктами, а не рядками;
  * Є вбудовані вектори слів;
  * Використовує нейронні мережі для тренування моделей.

Мінуси:
  * Менш гнучка в порівнянні з NLTK;
  * Токенізація пропозицій повільніше, ніж в NLTK;
  * Підтримує невелику кількість мов.

#### scikit-learn 

Бібліотека scikit-learn надає реалізацію цілого ряду алгоритмів для навчання з учителем і навчання без учителя через інтерфейс для Python. Побудована поверх SciPy. Орієнтована в першу чергу на моделювання даних, має досить функцій, щоб використовуватися для NLP в зв'язці з іншими бібліотеками.

Плюси:
  * Велика кількість алгоритмів для побудови моделей;
  * Містить функції для роботи з Bag-of-Words моделлю;
  * Гарна документація.

Мінуси:
  * Поганий препроцессінг, що змушує використовувати її в зв'язці з іншою бібліотекою (наприклад, NLTK);
  * Не використовує нейронні мережі для препроцессінга тексту.

#### gensim 

Python бібліотека для моделювання, тематичного моделювання документів і вилучення подібності для великих корпусів. У gensim реалізовані популярні NLP алгоритми, наприклад, word2vec. Більшість реалізацій можуть використовувати кілька ядер.

Плюси:
  * Працює з великими датасетами;
  * Підтримує глибоке навчання;
  * word2vec, tf-idf vectorization, document2vec.

Мінуси:
  * Заточена під моделі без вчителя;
  * Не містить достатнього функціоналу, необхідного для NLP, що змушує використовувати її разом з іншими бібліотеками.

Балто-слов'янські мови мають складну морфологію, що може погіршити якість обробки тексту, а також обмежити використання ряду бібліотек. Для роботи зі специфічною російської морфологією можна використовувати, наприклад, морфологічний аналізатор pymorphy2 і бібліотеку для пошуку і вилучення іменованих сутностей Natasha 
